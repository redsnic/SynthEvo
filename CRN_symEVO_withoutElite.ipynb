{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup done!"
     ]
    }
   ],
   "source": [
    "include(\"CRNevo.jl\")\n",
    "include(\"CRNExplore.jl\")\n",
    "include(\"SymbolicOps.jl\")\n",
    "\n",
    "# --- GA parameters\n",
    "genetic_pool_size = 1000\n",
    "\n",
    "death_rate = (rank) -> 0.0*(rank)\n",
    "mutation_rate = (rank) -> 0.50\n",
    "gradient_mutation_rate = (rank) -> 0.02*(1-rank)\n",
    "duplication_rate = (rank) -> 0.48*(1-rank)\n",
    "crossover_rate = (rank) -> 0.0*(1-rank)\n",
    "max_generations = 1000\n",
    "p_cross = 0.1\n",
    "\n",
    "# --- GD parameters\n",
    "\n",
    "# fixed, basic steps\n",
    "\n",
    "N = 3\n",
    "np = count_parameters(N)\n",
    "target = N\n",
    "crn = create_reactions(N)\n",
    "ode_crn = convert(ODESystem, crn)\n",
    "pars_l = assemble_opt_parameters_and_varables([0 for _ in 1:np], N) # just for the names\n",
    "ext_ode = make_sensitivity_ode(ode_crn, pars_l.p)\n",
    "t0 = 10.0\n",
    "t1 = 20.0\n",
    "\n",
    "# set up gd problem \n",
    "\n",
    "crn_info = (\n",
    "    crn = crn, \n",
    "    ode_crn = ode_crn,\n",
    "    ext_ode = ext_ode,\n",
    "    np = np,\n",
    "    N = N,\n",
    "    target = target\n",
    ")\n",
    "\n",
    "# set up the loss function\n",
    "\n",
    "gd_loss_options = (\n",
    "    weights = [1000., 1., 1.0/80, 5], #[2.0, 1.0/3, 1.0/80, 100.0], #[1., 0.01, 0.01, 1.] # this descends smoothly\n",
    "    p=0.001,\n",
    "    d=0.5,\n",
    "    f_ss=0.5,\n",
    "    norm_for_sensitivity_loss = 1, \n",
    "    norm_for_ss_loss = 1,\n",
    "    norm_for_adaptation_loss = 1,\n",
    "    n_losses = 4\n",
    ")\n",
    "\n",
    "loss_blueprint = prepare_args(nothing, target, t0, t1, pars_l, gd_loss_options.weights, gd_loss_options.p, gd_loss_options.d, gd_loss_options.f_ss, gd_loss_options.norm_for_sensitivity_loss, gd_loss_options.norm_for_ss_loss, gd_loss_options.norm_for_adaptation_loss)\n",
    "\n",
    "gd_options = (\n",
    "    alpha = 0.1,\n",
    "    n_iter = 10,\n",
    "    use_pruning_heuristic = false,\n",
    "    clip_value = nothing,\n",
    "    use_gradient_normalization = false,\n",
    "    use_adagrad = true, # overrides use_adam!\n",
    "    use_adam = false,\n",
    "    use_random_perturbation = false,\n",
    "    verbose = false,\n",
    "    # placed here for convenience, should be in gd_loss_options\n",
    "    symbolic_derivatives_of_loss = compute_symbolic_derivatives_of_loss(total_loss_symbolic(loss_blueprint))\n",
    ")\n",
    "\n",
    "gd_perturbation_options = (\n",
    "    t0 = t0,\n",
    "    t1 = t1,\n",
    "    input = 1.,\n",
    "    perturbation = 1.,\n",
    "    K = 5,\n",
    "    perturbation_list = [-1., -0.75, -0.5, -0.25, 0.25, 0.5, 0.75, 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.],\n",
    "    loss_blueprint = loss_blueprint\n",
    ")\n",
    "\n",
    "\n",
    "mutate_with_GD = (p) -> symbolic_gradient_descent(p, crn_info, gd_options, gd_perturbation_options, gd_loss_options).parameters\n",
    "function make_ev_loss(crn_info, gd_options, gd_perturbation_options, gd_loss_options)\n",
    "    function ev_loss(p)\n",
    "        pars_l = assemble_opt_parameters_and_varables(p, crn_info.N)\n",
    "        solutions = run_with_fixed_perturbations(crn_info.crn, p, pars_l, gd_perturbation_options.input,  gd_perturbation_options.perturbation_list, gd_perturbation_options.t0, gd_perturbation_options.t1)\n",
    "        losses = zeros(length(solutions))\n",
    "        Threads.@threads for i in 1:length(solutions)\n",
    "            loss_args = update_args(solutions[i], crn_info.target, gd_perturbation_options.t0, gd_perturbation_options.t1, pars_l, gd_perturbation_options.loss_blueprint, gd_loss_options.p, gd_loss_options.d, gd_loss_options.f_ss) \n",
    "            losses[i] = total_loss_eval(loss_args).total.val\n",
    "        end\n",
    "        return sum(losses)/length(gd_perturbation_options.perturbation_list)\n",
    "    end\n",
    "    return ev_loss\n",
    "end\n",
    "loss_function = make_ev_loss(crn_info, gd_options, gd_perturbation_options, gd_loss_options)\n",
    "print(\"setup done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k_29, x_3 --> ∅"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reactions(crn)[30] # so we want to edit only parameter 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_degradation_parameter = 29\n",
    "minimal_degradation_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# println(\"Testing symbolic gradient descent\")\n",
    "# println(loss_function([1 for _ in 1:np]))\n",
    "# test_p = symbolic_gradient_descent([1 for _ in 1:np], crn_info, gd_options, gd_perturbation_options, gd_loss_options)\n",
    "# println(loss_function(test_p.parameters))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize the GA problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done"
     ]
    }
   ],
   "source": [
    "parameter_pool = [[rand() for _ in 1:np] for _ in 1:genetic_pool_size]\n",
    "is_updated = [false for _ in 1:genetic_pool_size]\n",
    "fitness = [0. for _ in 1:genetic_pool_size]\n",
    "\n",
    "dp = 0.05\n",
    "\n",
    "state = (pool = parameter_pool, is_updated = is_updated, fitness = fitness, history = (best_loss = [], mean_loss = []))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0%┣                                             ┫ 0/1.0k [00:00<00:00, -0s/it]\n",
      "0.1%┣                                         ┫ 1/1.0k [00:51<Inf:Inf, InfGs/it]\n",
      "0.2%┣                                          ┫ 2/1.0k [01:34<26:11:37, 94s/it]\n",
      "0.3%┣▏                                         ┫ 3/1.0k [02:07<17:36:08, 64s/it]\n"
     ]
    }
   ],
   "source": [
    "using ProgressBars\n",
    "max_generations = 1000\n",
    "for i in ProgressBar(1:max_generations)\n",
    "    state = symbolic_evolve_NFB(crn, loss_function, state, dp, genetic_pool_size, death_rate, mutation_rate, gradient_mutation_rate, mutate_with_GD, duplication_rate, crossover_rate, p_cross, output_degradation_parameter, minimal_degradation_rate)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(state.history.mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(state.history.best_loss, label=\"best loss\", xlabel=\"generation\", ylabel=\"loss\", title=\"Best loss vs generation\", lw=2, legend=:topright)\n",
    "plot!(state.history.mean_loss, label=\"mean loss\", lw=2, line=:dash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = argmin(state.fitness)\n",
    "println(\"Best loss: \", state.fitness[best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb = 10\n",
    "opt_index = 494 \n",
    "#opt_index = 10\n",
    "\n",
    "opt_pars_v = state.pool[opt_index]\n",
    "opt_pars_l = assemble_opt_parameters_and_varables(opt_pars_v, N)\n",
    "\n",
    "sol = run_extended_with_fixed_perturbations(ext_ode, opt_pars_l, 1., [ perturb ], t0, t1)[1]\n",
    "println(\"Optimal index: \", opt_index)\n",
    "println(\"Adaptation error: \", abs(sol(t0)[3] - sol(t1)[3]))\n",
    "println(\"sensitivity: \", abs(sol(t0)[3] - sol(t0+0.5)[3]), \" and  loss : \", abs(abs(sol(t0)[3] - sol(t0+0.5)[3]) - 0.01))\n",
    "plot(sol.t, vec2mat(sol.u)[:,1:4], label=[\"X1\" \"X2\" \"X3 (output)\" \"U\"], lw=1.5, title=\"Dynamics of the optimal particle\", xlabel=\"time\", ylabel=\"concentration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom on reaction 3\n",
    "plot(sol.t, vec2mat(sol.u)[:,3], label=\"X3 Output\", xlabel=\"time\", ylabel=\"concentration\", title=\"Dynamics of the optimal particle (zoom)\", lw=2, color=3)\n",
    "xlims!(t0-1., t0+2)\n",
    "ylims!(sol(t0-1)[3]-sol(t0-1)[3]*0.3, sol(t0+2)[3]+sol(t0+2)[3]*0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_options_for_output = (\n",
    "    alpha = gd_options.alpha,\n",
    "    n_iter = gd_options.n_iter,\n",
    "    use_pruning_heuristic = gd_options.use_pruning_heuristic,\n",
    "    clip_value = gd_options.clip_value,\n",
    "    use_gradient_normalization = gd_options.use_gradient_normalization,\n",
    "    use_adagrad = gd_options.use_adagrad, # overrides use_adam!\n",
    "    use_adam = gd_options.use_adam,\n",
    "    use_random_perturbation = gd_options.use_random_perturbation,\n",
    "    verbose = true,\n",
    "    # placed here for convenience, should be in gd_loss_options\n",
    "    symbolic_derivatives_of_loss = gd_options.symbolic_derivatives_of_loss\n",
    ")\n",
    "out = symbolic_gradient_descent(state.pool[best], crn_info, gd_options_for_output, gd_perturbation_options, gd_loss_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:length(out.loss_tape), out.loss_tape, label=\"loss\", xlabel=\"iteration\", ylabel=\"loss\", title=\"Loss vs iteration\", lw=2, legend=:bottomright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Symbolics\n",
    "using LinearAlgebra\n",
    "function joint_jacobian(i, j, jac, initial_conditions)\n",
    "    A_ij = substitute(jac[i, j], unsym_dict(initial_conditions))\n",
    "    return A_ij\n",
    "end\n",
    "\n",
    "opt_index = 494\n",
    "# opt_index = argmin(predictions)\n",
    "# opt_index = length(loss_tape)\n",
    "\n",
    "opt_pars_v = state.pool[opt_index]\n",
    "opt_pars_l = assemble_opt_parameters_and_varables(opt_pars_v, N)\n",
    "\n",
    "jac = Symbolics.substitute(calculate_jacobian(ode_crn), unsym_dict(opt_pars_l.p))\n",
    "\n",
    "perturb = 2.5\n",
    "steady_state_after_perturbation = run_extended_with_fixed_perturbations(ext_ode, opt_pars_l, 1., [ perturb ], t0, t1)[1](t1)[1:3]\n",
    "steady_state_after_perturbation = [\n",
    "    :x_1 => steady_state_after_perturbation[1],\n",
    "    :x_2 => steady_state_after_perturbation[2],\n",
    "    :x_3 => steady_state_after_perturbation[3]\n",
    "]\n",
    "\n",
    "A_21 = joint_jacobian(2, 1, jac, steady_state_after_perturbation)\n",
    "A_32 = joint_jacobian(3, 2, jac, steady_state_after_perturbation)\n",
    "A_22 = joint_jacobian(2, 2, jac, steady_state_after_perturbation)\n",
    "A_31 = joint_jacobian(3, 1, jac, steady_state_after_perturbation)\n",
    "\n",
    "println(\"A_21 = \", A_21)\n",
    "println(\"A_32 = \", A_32)\n",
    "println(\"A_22 = \", A_22)\n",
    "println(\"A_31 = \", A_31)\n",
    "println(\"A_22*A_31 = \", A_22*A_31)\n",
    "println(\"A_21*A_32 = \", A_21*A_32)\n",
    "println(\"A_22*A_31 - A_21*A_32 = \", A_22*A_31 - A_21*A_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homeostatic_coefs_A21_A32 = []\n",
    "homeostatic_coefs_A22_A31 = []\n",
    "homeostatic_coefs = []\n",
    "\n",
    "for par_set in state.pool\n",
    "    opt_pars_v = par_set\n",
    "    opt_pars_l = assemble_opt_parameters_and_varables(opt_pars_v, N)\n",
    "\n",
    "    jac = Symbolics.substitute(calculate_jacobian(ode_crn), unsym_dict(opt_pars_l.p))\n",
    "\n",
    "    perturb = 2.5\n",
    "    steady_state_after_perturbation = run_extended_with_fixed_perturbations(ext_ode, opt_pars_l, 1., [ perturb ], t0, t1)[1](t1)[1:3]\n",
    "    steady_state_after_perturbation = [\n",
    "        :x_1 => steady_state_after_perturbation[1],\n",
    "        :x_2 => steady_state_after_perturbation[2],\n",
    "        :x_3 => steady_state_after_perturbation[3]\n",
    "    ]\n",
    "\n",
    "    A_21 = joint_jacobian(2, 1, jac, steady_state_after_perturbation)\n",
    "    A_32 = joint_jacobian(3, 2, jac, steady_state_after_perturbation)\n",
    "    A_22 = joint_jacobian(2, 2, jac, steady_state_after_perturbation)\n",
    "    A_31 = joint_jacobian(3, 1, jac, steady_state_after_perturbation)\n",
    "\n",
    "    push!(homeostatic_coefs_A21_A32, A_21*A_32)\n",
    "    push!(homeostatic_coefs_A22_A31, A_22*A_31)\n",
    "    push!(homeostatic_coefs, A_22*A_31 - A_21*A_32)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram([x.val for x in homeostatic_coefs], label=false, xlabel=\"Homeostatic coefficient\", ylabel=\"frequency\", title=\"Homeostatic coefficient distribution\", lw=0.5, legend=:bottomright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram([x.val for x in homeostatic_coefs_A21_A32], label=false, xlabel=\"A_21*A_32\", ylabel=\"frequency\", title=\"Homeostatic coefficient distribution\", lw=1, legend=:bottomright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram([x.val for x in homeostatic_coefs_A22_A31], label=false, xlabel=\"A_21*A_32\", ylabel=\"frequency\", title=\"Homeostatic coefficient distribution\", lw=0.5, legend=:bottomright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram([ x.val for x in homeostatic_coefs_A21_A32 + homeostatic_coefs_A22_A31], xlabel=\"A21_A32 + A22_A31\", ylabel=\"frequency\", title=\"Homeostatic coefficient distribution\", lw=1, legend=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([x.val for x in homeostatic_coefs_A22_A31], [x.val for x in homeostatic_coefs_A21_A32], seriestype = :scatter, xlabel=\"A_22*A_31\", ylabel=\"A_21*A_32\", title=\"Components of the homeostatic coefficient\", lw=2, legend=:bottomright, alpha=0.5)\n",
    "# xlims!(-85,0)\n",
    "# ylims!(-60,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findall(x -> 0.01 < x < 0.1, [x.val for x in homeostatic_coefs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(state.pool[494] .> 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_index_options = [best, rand(1:length(state.pool)), 494]\n",
    "opt_index_options_labels = [\"min loss\", \"$(opt_index_options[2])\", \"494\"]\n",
    "opt_index_options_colors = [:red, :green, :blue]\n",
    "\n",
    "hists = []\n",
    "for i in 1:length(opt_index_options)\n",
    "    opt_index = opt_index_options[i]\n",
    "    opt_pars_v = state.pool[opt_index]\n",
    "    opt_pars_l = assemble_opt_parameters_and_varables(opt_pars_v, 3)\n",
    "    h = histogram(opt_pars_v, label=opt_index_options_labels[i], xlabel=\"Parameter value\", ylabel=\"Frequency\", title=opt_index_options_labels[i], lw=2, legend=false, color=opt_index_options_colors[i], bins=20)\n",
    "    push!(hists, h)\n",
    "end\n",
    "plot(hists..., layout=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2\n",
    "\n",
    "save_object(\"NFB-finding-run-1000.jld2\", Dict(\n",
    "    \"state\" => state,\n",
    "    # all parameters \n",
    "    \"ga_options\" => Dict(\n",
    "        genetic_pool_size => genetic_pool_size,\n",
    "        death_rate => death_rate,\n",
    "        mutation_rate => mutation_rate,\n",
    "        gradient_mutation_rate => gradient_mutation_rate,\n",
    "        duplication_rate => duplication_rate,\n",
    "        crossover_rate => crossover_rate,\n",
    "        max_generations => max_generations,\n",
    "        p_cross => p_cross\n",
    "    ),\n",
    "    \"crn_info\" => crn_info,\n",
    "    \"gd_loss_options\" => gd_loss_options,\n",
    "    \"gd_options\" => gd_options,\n",
    "    \"gd_perturbation_options\" => gd_perturbation_options\n",
    "))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_load = load_object(\"NFB-finding-run-1000.jld2\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
